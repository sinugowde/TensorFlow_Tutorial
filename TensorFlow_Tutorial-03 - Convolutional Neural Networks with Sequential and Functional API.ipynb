{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Tutorial-03 - Convolutional Neural Networks with Sequential and Functional API",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+F6JakSGG7y/VwaR2A5Bq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFzVooPgdR1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "DEjrhtXHidUb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference\n",
        "# http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/"
      ],
      "metadata": {
        "id": "pLP5QIhgk8OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# convert the data types to float 32 bit & then normalize between 0~1\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0"
      ],
      "metadata": {
        "id": "QmLmvQYJi8kd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement with Sequential API"
      ],
      "metadata": {
        "id": "GrVqdUvHmflQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "     keras.Input(shape=(32, 32, 3)),\n",
        "     layers.Conv2D(32, 3, padding='valid', activation='relu'),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Conv2D(64, 3, activation='relu'),\n",
        "     layers.MaxPool2D(),\n",
        "     layers.Conv2D(128, 3, activation='relu'),\n",
        "     layers.Flatten(),\n",
        "     layers.Dense(64, activation='relu'),\n",
        "     layers.Dense(10)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chTUjjVYmWvA",
        "outputId": "6ef0fa0d-d442-4917-cfc2-eee3e3d34e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                131136    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_MCDA7Toevf",
        "outputId": "6334b0c8-f37b-41c5-fc1b-4f3a1bede28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 65s - loss: 1.6626 - accuracy: 0.3941 - 65s/epoch - 83ms/step\n",
            "Epoch 2/10\n",
            "782/782 - 70s - loss: 1.3485 - accuracy: 0.5158 - 70s/epoch - 90ms/step\n",
            "Epoch 3/10\n",
            "782/782 - 63s - loss: 1.2246 - accuracy: 0.5684 - 63s/epoch - 81ms/step\n",
            "Epoch 4/10\n",
            "782/782 - 63s - loss: 1.1226 - accuracy: 0.6064 - 63s/epoch - 81ms/step\n",
            "Epoch 5/10\n",
            "782/782 - 63s - loss: 1.0377 - accuracy: 0.6361 - 63s/epoch - 80ms/step\n",
            "Epoch 6/10\n",
            "782/782 - 64s - loss: 0.9755 - accuracy: 0.6598 - 64s/epoch - 81ms/step\n",
            "Epoch 7/10\n",
            "782/782 - 63s - loss: 0.9234 - accuracy: 0.6791 - 63s/epoch - 81ms/step\n",
            "Epoch 8/10\n",
            "782/782 - 63s - loss: 0.8757 - accuracy: 0.6968 - 63s/epoch - 80ms/step\n",
            "Epoch 9/10\n",
            "782/782 - 63s - loss: 0.8305 - accuracy: 0.7124 - 63s/epoch - 81ms/step\n",
            "Epoch 10/10\n",
            "782/782 - 63s - loss: 0.7919 - accuracy: 0.7274 - 63s/epoch - 81ms/step\n",
            "157/157 - 4s - loss: 0.9110 - accuracy: 0.6929 - 4s/epoch - 23ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9110416769981384, 0.6929000020027161]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the above model has just 3 convolution layers plus 10 epochs & with this, we see around 73% of training accuracy & around 70% accuracy on validation.\n",
        "# So this accuracy can be increased:\n",
        "# - increase the number of epochs\n",
        "# - increase the number of convolution layers"
      ],
      "metadata": {
        "id": "kb5fIEg4vNt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement with Functional API"
      ],
      "metadata": {
        "id": "jM-0dZw4w673"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets do it with function\n",
        "def functional_model():\n",
        "  inputs = keras.Input(shape=(32, 32, 3))\n",
        "  x = layers.Conv2D(32, 3)(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(64, 5, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Conv2D(128, 3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "Y8ge7ILIxElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = functional_model()\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGugEO9M34wY",
        "outputId": "49053cc4-da5e-4d30-e87a-d910fcb0c706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 254s - loss: 1.3255 - accuracy: 0.5254 - 254s/epoch - 325ms/step\n",
            "Epoch 2/10\n",
            "782/782 - 254s - loss: 0.9069 - accuracy: 0.6816 - 254s/epoch - 325ms/step\n",
            "Epoch 3/10\n",
            "782/782 - 258s - loss: 0.7493 - accuracy: 0.7366 - 258s/epoch - 330ms/step\n",
            "Epoch 4/10\n",
            "782/782 - 251s - loss: 0.6430 - accuracy: 0.7749 - 251s/epoch - 321ms/step\n",
            "Epoch 5/10\n",
            "782/782 - 252s - loss: 0.5542 - accuracy: 0.8081 - 252s/epoch - 322ms/step\n",
            "Epoch 6/10\n",
            "782/782 - 252s - loss: 0.4740 - accuracy: 0.8360 - 252s/epoch - 322ms/step\n",
            "Epoch 7/10\n",
            "782/782 - 254s - loss: 0.4096 - accuracy: 0.8586 - 254s/epoch - 325ms/step\n",
            "Epoch 8/10\n",
            "782/782 - 259s - loss: 0.3472 - accuracy: 0.8814 - 259s/epoch - 331ms/step\n",
            "Epoch 9/10\n",
            "782/782 - 261s - loss: 0.2877 - accuracy: 0.9052 - 261s/epoch - 334ms/step\n",
            "Epoch 10/10\n",
            "782/782 - 256s - loss: 0.2385 - accuracy: 0.9201 - 256s/epoch - 327ms/step\n",
            "157/157 - 12s - loss: 0.8896 - accuracy: 0.7345 - 12s/epoch - 79ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8895913362503052, 0.734499990940094]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with the above functional model, we have introduced the batch normalization and with this we see training accuracy is increased to around 92% but the validation accuracy is around 72%. \n",
        "# Here we still see there is gap between training & validation accuracies & see we have overfit in the training & needs to be removed may be by introducind regularization.\n",
        "# So lets reuse the above functional_model method to introduce the couple of regularization techniques in our model & create a new method for that below.\n",
        "# we will introduce L2 regularization alogn with dropout in this case."
      ],
      "metadata": {
        "id": "LY-PXSCPENMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "ny2OuWrywKrT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# introducing L2 & Dropout regularizations\n",
        "\n",
        "def functional_model_regularization():\n",
        "  inputs = keras.Input(shape=(32, 32, 3))\n",
        "  x = layers.Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(64, 5, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "pJZKBU2ef0hS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = functional_model_regularization()\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "print(\"Model Traing:\")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=250, verbose=2)\n",
        "print(\"Model Validation:\")\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "w0Bx4NHImH5D",
        "outputId": "c57e2039-6db7-430b-8df3-f950b5a90160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 6s - loss: 3.1273 - accuracy: 0.1373 - 6s/epoch - 8ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 5s - loss: 2.2851 - accuracy: 0.1457 - 5s/epoch - 7ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 5s - loss: 2.1839 - accuracy: 0.1736 - 5s/epoch - 7ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 5s - loss: 2.1297 - accuracy: 0.1868 - 5s/epoch - 7ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 5s - loss: 2.0502 - accuracy: 0.2098 - 5s/epoch - 7ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 5s - loss: 2.0274 - accuracy: 0.2135 - 5s/epoch - 7ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 5s - loss: 2.0165 - accuracy: 0.2191 - 5s/epoch - 7ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 5s - loss: 2.0044 - accuracy: 0.2181 - 5s/epoch - 7ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 5s - loss: 1.9934 - accuracy: 0.2241 - 5s/epoch - 7ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 5s - loss: 1.9889 - accuracy: 0.2267 - 5s/epoch - 7ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 5s - loss: 1.9802 - accuracy: 0.2281 - 5s/epoch - 7ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 5s - loss: 1.9760 - accuracy: 0.2304 - 5s/epoch - 7ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 5s - loss: 1.9652 - accuracy: 0.2337 - 5s/epoch - 7ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 5s - loss: 1.9656 - accuracy: 0.2330 - 5s/epoch - 7ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 5s - loss: 1.9593 - accuracy: 0.2368 - 5s/epoch - 7ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 5s - loss: 1.9572 - accuracy: 0.2374 - 5s/epoch - 7ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 5s - loss: 1.9578 - accuracy: 0.2387 - 5s/epoch - 7ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 5s - loss: 1.9478 - accuracy: 0.2443 - 5s/epoch - 7ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 5s - loss: 1.9416 - accuracy: 0.2454 - 5s/epoch - 7ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 5s - loss: 1.9404 - accuracy: 0.2436 - 5s/epoch - 7ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 5s - loss: 1.9386 - accuracy: 0.2431 - 5s/epoch - 7ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 5s - loss: 1.9373 - accuracy: 0.2457 - 5s/epoch - 7ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 5s - loss: 1.9389 - accuracy: 0.2463 - 5s/epoch - 7ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 5s - loss: 1.9329 - accuracy: 0.2461 - 5s/epoch - 7ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 5s - loss: 1.9289 - accuracy: 0.2459 - 5s/epoch - 7ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 5s - loss: 1.9345 - accuracy: 0.2463 - 5s/epoch - 7ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 5s - loss: 1.9256 - accuracy: 0.2469 - 5s/epoch - 7ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 5s - loss: 1.9271 - accuracy: 0.2485 - 5s/epoch - 7ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 5s - loss: 1.9253 - accuracy: 0.2498 - 5s/epoch - 7ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 5s - loss: 1.9237 - accuracy: 0.2502 - 5s/epoch - 7ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 5s - loss: 1.9237 - accuracy: 0.2514 - 5s/epoch - 7ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 5s - loss: 1.9206 - accuracy: 0.2529 - 5s/epoch - 7ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 5s - loss: 1.9172 - accuracy: 0.2515 - 5s/epoch - 7ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 5s - loss: 1.9228 - accuracy: 0.2513 - 5s/epoch - 7ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 5s - loss: 1.9192 - accuracy: 0.2540 - 5s/epoch - 7ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 5s - loss: 1.9172 - accuracy: 0.2530 - 5s/epoch - 7ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 5s - loss: 1.9189 - accuracy: 0.2537 - 5s/epoch - 7ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 5s - loss: 1.9135 - accuracy: 0.2556 - 5s/epoch - 7ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 5s - loss: 1.9142 - accuracy: 0.2547 - 5s/epoch - 7ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 5s - loss: 1.9146 - accuracy: 0.2541 - 5s/epoch - 7ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 5s - loss: 1.9066 - accuracy: 0.2582 - 5s/epoch - 7ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 5s - loss: 1.9133 - accuracy: 0.2548 - 5s/epoch - 7ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 5s - loss: 1.9110 - accuracy: 0.2576 - 5s/epoch - 7ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 5s - loss: 1.9091 - accuracy: 0.2577 - 5s/epoch - 7ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 5s - loss: 1.9077 - accuracy: 0.2558 - 5s/epoch - 7ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 5s - loss: 1.9066 - accuracy: 0.2584 - 5s/epoch - 7ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 5s - loss: 1.9049 - accuracy: 0.2581 - 5s/epoch - 7ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 5s - loss: 1.9075 - accuracy: 0.2592 - 5s/epoch - 7ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 5s - loss: 1.9024 - accuracy: 0.2606 - 5s/epoch - 7ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 5s - loss: 1.9032 - accuracy: 0.2610 - 5s/epoch - 7ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 5s - loss: 1.9013 - accuracy: 0.2608 - 5s/epoch - 7ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 5s - loss: 1.9025 - accuracy: 0.2619 - 5s/epoch - 7ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 5s - loss: 1.8974 - accuracy: 0.2606 - 5s/epoch - 7ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 5s - loss: 1.9043 - accuracy: 0.2617 - 5s/epoch - 7ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 5s - loss: 1.8961 - accuracy: 0.2638 - 5s/epoch - 7ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 5s - loss: 1.9058 - accuracy: 0.2595 - 5s/epoch - 7ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 5s - loss: 1.9001 - accuracy: 0.2617 - 5s/epoch - 7ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 5s - loss: 1.8996 - accuracy: 0.2612 - 5s/epoch - 7ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 5s - loss: 1.9105 - accuracy: 0.2604 - 5s/epoch - 7ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 5s - loss: 1.8977 - accuracy: 0.2631 - 5s/epoch - 7ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 5s - loss: 1.9022 - accuracy: 0.2603 - 5s/epoch - 7ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 5s - loss: 1.8945 - accuracy: 0.2630 - 5s/epoch - 7ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 5s - loss: 1.9054 - accuracy: 0.2622 - 5s/epoch - 7ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 5s - loss: 1.8925 - accuracy: 0.2646 - 5s/epoch - 7ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 5s - loss: 1.8938 - accuracy: 0.2660 - 5s/epoch - 7ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 5s - loss: 1.9004 - accuracy: 0.2615 - 5s/epoch - 7ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 5s - loss: 1.8979 - accuracy: 0.2638 - 5s/epoch - 7ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 5s - loss: 1.8898 - accuracy: 0.2657 - 5s/epoch - 7ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 5s - loss: 1.8926 - accuracy: 0.2669 - 5s/epoch - 7ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 5s - loss: 1.8968 - accuracy: 0.2636 - 5s/epoch - 7ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 5s - loss: 1.8953 - accuracy: 0.2634 - 5s/epoch - 7ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 5s - loss: 1.8876 - accuracy: 0.2677 - 5s/epoch - 7ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 5s - loss: 1.8932 - accuracy: 0.2633 - 5s/epoch - 7ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 5s - loss: 1.8841 - accuracy: 0.2689 - 5s/epoch - 7ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 5s - loss: 1.8907 - accuracy: 0.2654 - 5s/epoch - 7ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 5s - loss: 1.8854 - accuracy: 0.2682 - 5s/epoch - 7ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 5s - loss: 1.8952 - accuracy: 0.2659 - 5s/epoch - 7ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 5s - loss: 1.8879 - accuracy: 0.2666 - 5s/epoch - 7ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 5s - loss: 1.8927 - accuracy: 0.2655 - 5s/epoch - 7ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 5s - loss: 1.8903 - accuracy: 0.2660 - 5s/epoch - 7ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 5s - loss: 1.8849 - accuracy: 0.2699 - 5s/epoch - 7ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 5s - loss: 1.8924 - accuracy: 0.2670 - 5s/epoch - 7ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 5s - loss: 1.8895 - accuracy: 0.2673 - 5s/epoch - 7ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 5s - loss: 1.8810 - accuracy: 0.2704 - 5s/epoch - 7ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 5s - loss: 1.8823 - accuracy: 0.2693 - 5s/epoch - 7ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 5s - loss: 1.8808 - accuracy: 0.2692 - 5s/epoch - 7ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 5s - loss: 1.8872 - accuracy: 0.2667 - 5s/epoch - 7ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 5s - loss: 1.8926 - accuracy: 0.2657 - 5s/epoch - 7ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 5s - loss: 1.8853 - accuracy: 0.2703 - 5s/epoch - 7ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 5s - loss: 1.8842 - accuracy: 0.2696 - 5s/epoch - 7ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 5s - loss: 1.8825 - accuracy: 0.2702 - 5s/epoch - 7ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 5s - loss: 1.8892 - accuracy: 0.2690 - 5s/epoch - 7ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 5s - loss: 1.8769 - accuracy: 0.2705 - 5s/epoch - 7ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 5s - loss: 1.8833 - accuracy: 0.2726 - 5s/epoch - 7ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 5s - loss: 1.8793 - accuracy: 0.2718 - 5s/epoch - 7ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 5s - loss: 1.8856 - accuracy: 0.2718 - 5s/epoch - 7ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 5s - loss: 1.8828 - accuracy: 0.2715 - 5s/epoch - 7ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 5s - loss: 1.8796 - accuracy: 0.2730 - 5s/epoch - 7ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 5s - loss: 1.8823 - accuracy: 0.2728 - 5s/epoch - 7ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 5s - loss: 1.8882 - accuracy: 0.2713 - 5s/epoch - 7ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 5s - loss: 1.8830 - accuracy: 0.2713 - 5s/epoch - 7ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 5s - loss: 1.8804 - accuracy: 0.2712 - 5s/epoch - 7ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 5s - loss: 1.8906 - accuracy: 0.2708 - 5s/epoch - 7ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 5s - loss: 1.8794 - accuracy: 0.2719 - 5s/epoch - 7ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 5s - loss: 1.8879 - accuracy: 0.2698 - 5s/epoch - 7ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 5s - loss: 1.8867 - accuracy: 0.2688 - 5s/epoch - 7ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 5s - loss: 1.8859 - accuracy: 0.2724 - 5s/epoch - 7ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 5s - loss: 1.8783 - accuracy: 0.2731 - 5s/epoch - 7ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 5s - loss: 1.8828 - accuracy: 0.2711 - 5s/epoch - 7ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 5s - loss: 1.8776 - accuracy: 0.2725 - 5s/epoch - 7ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 5s - loss: 1.8834 - accuracy: 0.2726 - 5s/epoch - 7ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 5s - loss: 1.8835 - accuracy: 0.2711 - 5s/epoch - 7ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 5s - loss: 1.8816 - accuracy: 0.2718 - 5s/epoch - 7ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 5s - loss: 1.8830 - accuracy: 0.2730 - 5s/epoch - 7ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 5s - loss: 1.8851 - accuracy: 0.2709 - 5s/epoch - 7ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 5s - loss: 1.8772 - accuracy: 0.2729 - 5s/epoch - 7ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 5s - loss: 1.8789 - accuracy: 0.2730 - 5s/epoch - 7ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 5s - loss: 1.8797 - accuracy: 0.2744 - 5s/epoch - 7ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 5s - loss: 1.8923 - accuracy: 0.2704 - 5s/epoch - 7ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 5s - loss: 1.8814 - accuracy: 0.2748 - 5s/epoch - 7ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 5s - loss: 1.8768 - accuracy: 0.2726 - 5s/epoch - 7ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 5s - loss: 1.8748 - accuracy: 0.2743 - 5s/epoch - 7ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 5s - loss: 1.8770 - accuracy: 0.2730 - 5s/epoch - 7ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 5s - loss: 1.8802 - accuracy: 0.2741 - 5s/epoch - 7ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 5s - loss: 1.8823 - accuracy: 0.2736 - 5s/epoch - 7ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 5s - loss: 1.8862 - accuracy: 0.2716 - 5s/epoch - 7ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 5s - loss: 1.8771 - accuracy: 0.2744 - 5s/epoch - 7ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 5s - loss: 1.8792 - accuracy: 0.2747 - 5s/epoch - 7ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 5s - loss: 1.8675 - accuracy: 0.2773 - 5s/epoch - 7ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 5s - loss: 1.8776 - accuracy: 0.2748 - 5s/epoch - 7ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 5s - loss: 1.8803 - accuracy: 0.2733 - 5s/epoch - 7ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 5s - loss: 1.8789 - accuracy: 0.2741 - 5s/epoch - 7ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 5s - loss: 1.8782 - accuracy: 0.2734 - 5s/epoch - 7ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 5s - loss: 1.8770 - accuracy: 0.2755 - 5s/epoch - 7ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 5s - loss: 1.8754 - accuracy: 0.2739 - 5s/epoch - 7ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 5s - loss: 1.8788 - accuracy: 0.2723 - 5s/epoch - 7ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 5s - loss: 1.8710 - accuracy: 0.2778 - 5s/epoch - 7ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 5s - loss: 1.8769 - accuracy: 0.2741 - 5s/epoch - 7ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 5s - loss: 1.8792 - accuracy: 0.2736 - 5s/epoch - 7ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 5s - loss: 1.8741 - accuracy: 0.2761 - 5s/epoch - 7ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 5s - loss: 1.8832 - accuracy: 0.2737 - 5s/epoch - 7ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 5s - loss: 1.8783 - accuracy: 0.2753 - 5s/epoch - 7ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 5s - loss: 1.8719 - accuracy: 0.2773 - 5s/epoch - 7ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 5s - loss: 1.8761 - accuracy: 0.2744 - 5s/epoch - 7ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 5s - loss: 1.8748 - accuracy: 0.2778 - 5s/epoch - 7ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 5s - loss: 1.8723 - accuracy: 0.2761 - 5s/epoch - 7ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 5s - loss: 1.8733 - accuracy: 0.2776 - 5s/epoch - 7ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 5s - loss: 1.8784 - accuracy: 0.2750 - 5s/epoch - 7ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 5s - loss: 1.8767 - accuracy: 0.2771 - 5s/epoch - 7ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 5s - loss: 1.8767 - accuracy: 0.2745 - 5s/epoch - 7ms/step\n",
            "157/157 - 1s - loss: 1.6439 - accuracy: 0.4041 - 673ms/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6439129114151, 0.4041000008583069]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}